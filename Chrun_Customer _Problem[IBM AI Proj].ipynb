{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. <U>Problem Statement</U>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An E Commerce company or DTH (you can choose either of these two domains) provider is facing a lot of competition in the current market and it has become a challenge to retain the existing customers in the current situation. Hence, the company wants to develop a model through which they can do churn prediction of the accounts and provide segmented offers to the potential churners. In this company, account churn is a major thing because 1 account can have multiple customers. hence by losing one account the company might be losing more than one customer.\n",
    "\n",
    "You have been assigned to develop a churn prediction model for this company and provide business recommendations on the campaign.\n",
    "\n",
    "Your campaign suggestion should be unique and be very clear on the campaign offer because your recommendation will go through the revenue assurance team. If they find that you are giving a lot of free (or subsidized) stuff thereby making a loss to the company; they are not going to approve your recommendation.\n",
    "\n",
    "Hence be very careful while providing campaign recommendation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. <U>Data Collection</U>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset is mentioned as below:\n",
    "\n",
    "Customer Churn Data.xlsx\n",
    "\n",
    "Dataset granted by: PARIKSHITH A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3. <U>Data Ingestion</U>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf=pd.read_excel('Customer Churn Data.xlsx',sheet_name='Data for DSBA') # reading the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.head() #top 5 rows get displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.tail() #last bottom 5 rows get displayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 4. <U>Data Cleaning </U>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for null values and duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.dtypes # data types of each individual column get printed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, there are 12 categorical characteristics in this list, 5 of which are float types and the remaining 7 are all integer types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.isnull().sum() # checking each individual column that there are Null values or not|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we'll replace the column with NULL values with the median and mode, i.e., categorical by mode and numerical by mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.duplicated().sum() #there no duplicated rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are in the process of discarding the columns which satisfy neither our purposes nor those of our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.drop(['AccountID','Payment','Gender','Marital_Status','cashback','Login_device'],axis=1,inplace=True) #dropping the unwanted column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.drop(['coupon_used_for_payment'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.dtypes # data types of each individual column get printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify object columns except for 'Column1'\n",
    "object_columns = cf.select_dtypes(include=['object']).columns.drop('account_segment')\n",
    "\n",
    "\n",
    "# Convert object columns to float using pd.to_numeric()\n",
    "cf[object_columns] = cf[object_columns].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf['Churn'] = cf['Churn'].astype(float) #converting int to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now substitute mean and mode for the NULL values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf = cf.fillna(cf.median()) #this is for numerical column\n",
    "\n",
    "numeric_columns = cf.select_dtypes(include=['float', 'int']).columns\n",
    "cf[numeric_columns] = cf[numeric_columns].fillna(cf[numeric_columns].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets check again that there is null counts or not\n",
    "cf.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets fill object column null values with mode\n",
    "object_columns = cf.select_dtypes(include=['object']).columns\n",
    "cf[object_columns] = cf[object_columns].fillna(cf[object_columns].mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets check again that there is null counts or not\n",
    "cf.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in each column\n",
    "for column in cf.columns:\n",
    "    unique_values = cf[column].unique()\n",
    "    print(f\"Unique values in {column}: {unique_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5. <U>Exploratory Data Analysis (EDA) </U>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.sample(6) #this line will randomly select any five column from dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### <em>Attributes Information:</em>\n",
    ">- churn: account churn flag (Target)\n",
    ">- Tenure:Tenure of account\n",
    ">- City_Tier:Tier of primary customer's city\n",
    ">- cc_contacted_Ly:How many time the customer of the account has contacted customer care in last 12months\n",
    ">- Service_Score:Satisfaction score given by customers of the account on service provided by company\n",
    ">- Account_user_count:Number of customers tagged with this account\n",
    ">- account_segment:Account segmentation on the basis of spend\n",
    ">- CC_Agent_Score:Satisfaction score given by customers of the account on customer care service provided by company\n",
    ">- rev_per_month:Monthly average revenue generated by account in last 12 months\n",
    ">- complain_ly:Any complaints has been raised by account in last 12 months\n",
    ">- rev_growth_yoy:revenue growth percentage of the account (last 12 months vs last 24 to 13month)\n",
    ">- Day_Since_CC_connect:Number of days since no customers in the account has contacted the customer care\n",
    ">\n",
    "> .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.shape #{checking the number of rows and column}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.info() #{ checking the dataypes and count of null values if present}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf['Churn'].value_counts() # Here it can be seen that the number of 1's is very less as compared to number of 0's. So that dataset is imbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the number of 1s is much lower than the number of 0s. As a result, that dataset is imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <em> Separating Numerical and Categorical Features </em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feature= [fea for fea in cf.columns if cf[fea].dtype !=object]\n",
    "cat_feature= [fea for fea in cf.columns if cf[fea].dtype==object]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have {} Numerical features : {}\".format(len(num_feature),num_feature))\n",
    "print()\n",
    "print(\"We have {} Categorical features : {}\".format(len(cat_feature),cat_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,17))\n",
    "plt.suptitle('Univariate Analysis',fontsize=20,fontweight='bold',y=1)\n",
    "\n",
    "for i in range(0,len(num_feature)):\n",
    "    plt.subplot(5,3,i+1)\n",
    "    sns.kdeplot(x=cf[num_feature[i]],shade=True,color='g')\n",
    "    plt.xlabel(num_feature[i])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Observations\n",
    ">- CC_Agent_scored are normally distributed.\n",
    ">- Tenure,Day_Since_CC_connect,rev_per_month,CC_Contacted_ly, is havily left skewed.\n",
    ">- Almost 50% of the customefr are not active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,17)) #fixing the size of graph\n",
    "plt.suptitle('Scatter plot for Numerical features',fontsize=20,fontweight='bold',y=1) # placing at top of the graph heading.\n",
    "\n",
    "for i in range(0,len(num_feature)):\n",
    "    plt.subplot(5,3,i+1)\n",
    "    sns.scatterplot(y=num_feature[i],x=cf.index,data=cf,color='b',hue='Churn')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,40))\n",
    "plt.suptitle('Count plot for Categorical features',fontsize=20,fontweight='bold',y=1)\n",
    "\n",
    "for i in range(0,len(cat_feature)):\n",
    "    plt.subplot(8,2,i+1)\n",
    "    sns.countplot(y=cat_feature[i],data=cf)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,20))\n",
    "sns.pairplot(cf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bivariate Analysis w.r.t Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,17))\n",
    "plt.suptitle('Bivariate Analysis w.r.t Target Variable',fontsize=20,fontweight='bold',y=1)\n",
    "\n",
    "for i in range(0,len(num_feature)):\n",
    "    plt.subplot(5,3,i+1)\n",
    "    sns.barplot(y=num_feature[i],x='Churn',data=cf,ci=0)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns\n",
    "numeric_columns = cf.select_dtypes(include=['float', 'int'])\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = numeric_columns.corr()\n",
    "\n",
    "# Plot the correlation matrix using a heatmap\n",
    "plt.figure(figsize=(15, 14))\n",
    "sns.heatmap(corr_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.histplot(x='Day_Since_CC_connect',hue='Churn',data=cf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=cf.Churn,hue=cf.City_Tier,color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.countplot(x='Churn',hue='Service_Score',data=cf,color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,30))\n",
    "plt.suptitle('BOX PLOT for all numerical feature',fontsize=20,fontweight='bold',y=1)\n",
    "\n",
    "for i in range(0,len(num_feature)):\n",
    "    plt.subplot(8,2,i+1)\n",
    "    sns.boxplot(data=cf[num_feature],orient='h') # checking the outliers are present or not\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 6. <U>Data Preprocessing</U>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature encoding to convert the categorical features into numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.head() #checking top 5 rows of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now doing one_hot_encoding to convert categorical features into numerical feature\n",
    "cf1=pd.get_dummies(data=cf,columns=['account_segment'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the target column into separate vectors for training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cf1.drop(['Churn'],axis=1) #dropping the label\n",
    "Y=cf1['Churn']\n",
    "X.shape,Y.shape #checking shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the dataset by using oversampling technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "smk = SMOTETomek()\n",
    "X_res, Y_res = smk.fit_resample(X, Y)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res.shape,Y_res.shape #after balancing the data set checking new shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <em> Splitting the dataset into train and test data </em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X_res,Y_res,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,X_test.shape,Y_train.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train,Y_train)\n",
    "Y_predict=model.predict(X_test)\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score=model.score(X_test,Y_test)\n",
    "print('Accuracy score is',model_score*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mat = confusion_matrix(Y_test, Y_predict) # Calculate the confusion matrix\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_mat)  # Print the confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <em> Buiding the Model </em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import export_graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style\n",
    "plt.style.use('classic')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "'criterion': ['gini'],\n",
    "'max_depth': [10],#[10,20,30,50]#[3,5,7,9]\n",
    "'min_samples_leaf': [250],#[100,150,200,250]# 1-3% 50-150\n",
    "'min_samples_split': [750]#[150,300,450,600,750] # 150 - 450\n",
    "}\n",
    "dtcl = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtcl, param_grid = param_grid, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, Y_train)\n",
    "print(grid_search.best_params_)\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid\n",
    "#{'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 250, 'min_samples_split': 750}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_predict = best_grid.predict(X_train)\n",
    "ytest_predict = best_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <em>Predicting on Training and Test dataset</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest_predict\n",
    "ytest_predict_prob=best_grid.predict_proba(X_test)\n",
    "ytest_predict_prob\n",
    "pd.DataFrame(ytest_predict_prob).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <em>Model Evaluation</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_train, ytrain_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Data Accuracy\n",
    "cart_train_acc=best_grid.score(X_train,Y_train)\n",
    "cart_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_train, ytrain_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_metrics=classification_report(Y_train, ytrain_predict,output_dict=True)\n",
    "cf1=pd.DataFrame(cart_metrics).transpose()\n",
    "cart_train_precision=round(cf1.loc[\"1.0\"][0],2)\n",
    "cart_train_recall=round(cf1.loc[\"1.0\"][1],2)\n",
    "cart_train_f1=round(cf1.loc[\"1.0\"][2],2)\n",
    "print ('cart_train_precision ',cart_train_precision)\n",
    "print ('cart_train_recall ',cart_train_recall)\n",
    "print ('cart_train_f1 ',cart_train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for the Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_test, ytest_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Data Accuracy\n",
    "cart_test_acc=best_grid.score(X_test,Y_test)\n",
    "cart_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, ytest_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_metrics=classification_report(Y_test, ytest_predict,output_dict=True)\n",
    "cf1=pd.DataFrame(cart_metrics).transpose()\n",
    "cart_test_precision=round(cf1.loc[\"1.0\"][0],2)\n",
    "cart_test_recall=round(cf1.loc[\"1.0\"][1],2)\n",
    "cart_test_f1=round(cf1.loc[\"1.0\"][2],2)\n",
    "print ('cart_train_precision ',cart_test_precision)\n",
    "print ('cart_train_recall ',cart_test_recall)\n",
    "print ('cart_train_f1 ',cart_test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <em>Building a Random Forest Classifier</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search for finding out the optimal values for the hyper parameters\n",
    "\n",
    "Due to large volume of data, trying for different parameter values in the gridsearch with higher cv value will lead\n",
    "to performance issues and model will run for much longer time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {#put a grid for hyperparameters\n",
    "'max_depth': [8,9],\n",
    "'max_features': [8,9],#[5,4,6],\n",
    "'min_samples_leaf': [250,150],\n",
    "'min_samples_split': [750,500],\n",
    "'n_estimators': [100,150]#150,250\n",
    "}\n",
    "rfcl = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator = rfcl, param_grid = param_grid, cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "best_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <em>Predicting the Training and Testing data </em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_predict = best_grid.predict(X_train)\n",
    "ytest_predict = best_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <em>RF Model Perfotrmance Evaluation on Training data</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_train,ytrain_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train_acc=best_grid.score(X_train,Y_train)\n",
    "rf_train_acc\n",
    "rf_test_acc=best_grid.score(X_test,Y_test)\n",
    "rf_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_train,ytrain_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_metrics=classification_report(Y_train, ytrain_predict,output_dict=True)\n",
    "cf1=pd.DataFrame(rf_metrics).transpose()\n",
    "rf_train_precision=round(cf1.loc[\"1.0\"][0],2)\n",
    "rf_train_recall=round(cf1.loc[\"1.0\"][1],2)\n",
    "rf_train_f1=round(cf1.loc[\"1.0\"][2],2)\n",
    "rf_test_recall=round(cf1.loc[\"1.0\"][1],2)\n",
    "print ('rf_train_precision ',rf_train_precision)\n",
    "print ('rf_train_recall ',rf_train_recall)\n",
    "print ('rf_train_f1 ',rf_train_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train_fpr, rf_train_tpr,_=roc_curve(Y_train,best_grid.predict_proba(X_train)[:,1])\n",
    "plt.plot(rf_train_fpr,rf_train_tpr,color='green')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "rf_train_auc=roc_auc_score(Y_train,best_grid.predict(X_train))\n",
    "print('Area under Curve is', rf_train_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <em>Building a Neural Network Classifier</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "'hidden_layer_sizes': [64,128],#[32,64,128], #Multiple layers(200,120)\n",
    "'max_iter': [100,200,300],\n",
    "'solver': ['adam','sgd'], #sgd\n",
    "}\n",
    "nncl = MLPClassifier(tol = 0.01)\n",
    "grid_search = GridSearchCV(estimator = nncl, param_grid = param_grid, cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train_scaled, Y_train)\n",
    "#{'hidden_layer_sizes': 32, 'max_iter': 200, 'solver': 'adam', 'tol': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "best_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <em>Predicting the Training and Testing data</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_predict = best_grid.predict(X_train_scaled)\n",
    "ytest_predict = best_grid.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <em> NN Model Performance Evaluation on Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_train,ytrain_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_train_acc=best_grid.score(X_train_scaled,Y_train)\n",
    "nn_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_train,ytrain_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_metrics=classification_report(Y_train, ytrain_predict,output_dict=True)\n",
    "cf1=pd.DataFrame(nn_metrics).transpose()\n",
    "nn_train_precision=round(cf1.loc[\"1.0\"][0],2)\n",
    "nn_train_recall=round(cf1.loc[\"1.0\"][1],2)\n",
    "nn_train_f1=round(cf1.loc[\"1.0\"][2],2)\n",
    "print ('nn_train_precision ',nn_train_precision)\n",
    "print ('nn_train_recall ',nn_train_recall)\n",
    "print ('nn_train_f1 ',nn_train_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_train_fpr, nn_train_tpr,_=roc_curve(Y_train,best_grid.predict_proba(X_train)[:,1])\n",
    "plt.plot(nn_train_fpr,nn_train_tpr,color='black')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "nn_train_auc=roc_auc_score(Y_train,best_grid.predict(X_train))\n",
    "print('Area under Curve is', nn_train_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <em>NN Model Performance Evaluation on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_test,ytest_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_test_acc=best_grid.score(X_test_scaled,Y_test)\n",
    "nn_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test,ytest_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_metrics=classification_report(Y_test, ytest_predict,output_dict=True)\n",
    "cf1=pd.DataFrame(nn_metrics).transpose()\n",
    "nn_test_precision=round(cf1.loc[\"1.0\"][0],2)\n",
    "nn_test_recall=round(cf1.loc[\"1.0\"][1],2)\n",
    "nn_test_f1=round(cf1.loc[\"1.0\"][2],2)\n",
    "print ('nn_test_precision ',nn_test_precision)\n",
    "print ('nn_test_recall ',nn_test_recall)\n",
    "print ('nn_test_f1 ',nn_test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_test_fpr, nn_test_tpr,_=roc_curve(Y_test,best_grid.predict_proba(X_test)[:,1])\n",
    "plt.plot(nn_test_fpr,nn_test_tpr,color='black')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "nn_test_auc=roc_auc_score(Y_test,best_grid.predict(X_test))\n",
    "print('Area under Curve is', nn_test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=['Accuracy','Recall','Precision','f1_Score']\n",
    "data=pd.DataFrame({'CART Train':[cart_train_acc,cart_train_recall,cart_train_precision,cart_train_f1],\n",
    "                    'CART Test':[cart_test_acc,cart_test_recall,cart_test_precision,cart_test_f1],\n",
    "                    'Random forest train':[rf_train_acc,rf_train_recall,rf_train_precision,rf_train_f1],\n",
    "                    'Neural Network train':[nn_train_acc,nn_train_recall,nn_train_precision,nn_train_f1],\n",
    "                    'Neural Network test':[nn_test_acc,nn_test_recall,nn_test_precision,nn_test_f1]},index=index)\n",
    "round(data,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <em>Based on the provided output from the dataset, here is an interpretation of the accuracy scores for different models:</em>\n",
    "\n",
    "####  1. Decision Tree (CART) Model:\n",
    "- Train Data Accuracy: The decision tree model achieved a training accuracy of approximately 84.82% (0.8482). This indicates that the model correctly predicted the churn outcome for about 84.82% of the training data.\n",
    "- Test Data Accuracy: The neural network model achieved a test accuracy of approximately 84.3% (0.8430). This suggests that the model correctly predicted the churn outcome for approximately 84.3% of the test data.\n",
    "\n",
    "#### 2. Random Forest Model:\n",
    "- Train Data Accuracy: The random forest model achieved a training accuracy of around 85.42% (0.8542). This suggests that the model correctly predicted the churn outcome for approximately 85.42% of the training data.\n",
    "\n",
    "#### 3. Neutral Network Model:\n",
    "- Train Data Accuracy: The neural network model achieved a training accuracy of approximately 88.24% (0.8824). This indicates that the model correctly predicted the churn outcome for around 88.24% of the training data.\n",
    "- Test Data Accuracy: The neural network model achieved a test accuracy of approximately 88.25% (0.8825). This suggests that the model correctly predicted the churn outcome for approximately 82.25% of the test data.\n",
    "\n",
    "### <em>Based on above observation our data set is good fit and good model</em>\n",
    "### <em>Based on these results, the neural network model seems to have the highest accuracy both on the training and test data, indicating its better performance compared to the decision tree and random forest models. However, it is important to consider other factors such as model complexity, interpretability, and potential overfitting when selecting the best model for your specific churnprediction task.</em>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
